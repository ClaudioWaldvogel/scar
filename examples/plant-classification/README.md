# Data-Driven Plants Classification with Lasagne/Theano on AWS Batch through SCAR

In the context of the [DEEP Hybrid-DataCloud](https://deep-hybrid-datacloud.eu/) European project a [Plants Classification](https://github.com/indigo-dc/plant-classification-theano) application based on Lasagne/Theano has been developed in order to recognize certain plant species in images.

The Docker image in Docker Hub [[deephdc/deep-oc-plant-classification-theano](https://hub.docker.com/r/deephdc/deep-oc-plant-classification-theano/) (~600 MB)] is [too big](https://scar.readthedocs.io/en/latest/limitations.html) to fit in AWS Lambda in order to be executed using SCAR. 

Still, it would be interesting to have a data-driven highly available service in the Cloud that features zero cost when no one is using it and would dynamically scale in order to cope with increased plants classification requests (when uploading an image to a certain folder).

Enter the [AWS Batch integration in SCAR](https://scar.readthedocs.io/en/latest/batch.html). This allows to delegate event-driven executions of jobs that do not fit in the AWS Lambda constraints into AWS Batch in order to be executed as containers out of the user-defined Docker images on Amazon EC2 instances.

Users upload the images to a certain input folder of a bucket and they receive the output data (in this case an image tagged with the classified plant) in a certain output folder. The automated elasticity is managed by AWS Batch. Scale to zero (nodes) is automatically enforced so that the service costs *zero* when it is not running any job.

## Deploying the Plants Classification event-driven service in AWS

In this case we will configure the Lambda function to automatically delegate to AWS Batch the jobs responsible to process every image upload. In order to bypass the 24KB limits of an AWS Batch job definition we will upload a script into S3 and pull the script from the running container. This is only required if you want to execute a customized script that is not present in the Docker image, as in this case with the [`classify_image.py`](https://github.com/grycap/oscar/blob/master/examples/plant-classification-theano/classify_image.py) python script. You may want to refresh the [SCAR programming model](https://scar.readthedocs.io/en/latest/prog_model.html) for further information.

1. Create the Lambda function

```sh
scar init -f scar-plant-classification.yaml
```

2. Upload the processing script to Amazon S3

```sh
scar put -b scar-test/scar-plants -p plant-classification-run.sh
```

3. Upload an image to the input folder of the bucket (to trigger the image processing)

```sh
scar put -b scar-test/scar-plants/input -p daisy.jpg
```

If you upload several files, the corresponding jobs will be submitted to AWS Batch. Depending on the number of jobs it may decide to spawn additional EC2 instances in order to cope with the increased workload. Once the executions have finished (you can check the corresponding CloudWatch logs) ...

4. Download the generated files from the S3 bucket

```sh
scar get -b scar-test/scar-plants/output -p /tmp/plant-classification
```

The last command creates an `output/$REQUEST_ID` folder in the `/tmp/plant-classification` path with the files generated by each invocation.
 